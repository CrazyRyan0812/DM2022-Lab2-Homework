{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: ÊùéÂ∫≠Á´∂\n",
    "\n",
    "Student ID: 111033626\n",
    "\n",
    "GitHub ID: CrazyRyan0812\n",
    "\n",
    "Kaggle name: CrazyRyan\n",
    "\n",
    "Kaggle private scoreboard snapshot:\n",
    "\n",
    "![Snapshot](img/pic0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home** exercises in the [DM2022-Lab2-master Repo](https://github.com/keziatamus/DM2022-Lab2-Master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/competitions/dm2022-isa5810-lab2-homework) regarding Emotion Recognition on Twitter by this link https://www.kaggle.com/t/2b0d14a829f340bc88d2660dc602d4bd. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (60-x)/6 + 20 points, where x is your ranking in the leaderboard (ie. If you rank 3rd your score will be (60-3)/6 + 20 = 29.5% out of 30%)   \n",
    "    Submit your last submission __BEFORE the deadline (Nov. 22th 11:59 pm, Tuesday)_. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developping the model for the competition (You can use code and comment it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook** and **add minimal comments where needed**.\n",
    "\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Nov. 25th 11:59 pm, Friday)__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# 1 Data Preperation\n",
    "## 1.1 read data to panda_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "data_identification = pd.read_csv(\"dm2022-isa5810-lab2-homework/data_identification.csv\", skiprows=1, header=None, names=[\"tweet_id\", \"identification\"])\n",
    "emotion = pd.read_csv(\"dm2022-isa5810-lab2-homework/emotion.csv\", skiprows=1, header=None, names=[\"tweet_id\", \"emotion\"])\n",
    "\n",
    "tweets_DM_json=[]\n",
    "for line in open('dm2022-isa5810-lab2-homework/tweets_DM.json', 'r'):\n",
    "    tweets_DM_json.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_tweets = [(tw['_source']['tweet']['tweet_id'],tw['_source']['tweet']['text'],tw['_source']['tweet']['hashtags'],tw['_score']) for tw in tweets_DM_json]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha üòÇüòÇüòÇ &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>[]</td>\n",
       "      <td>989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>0x316b80</td>\n",
       "      <td>When you buy the last 2 tickets remaining for ...</td>\n",
       "      <td>[mixedfeeling, butimTHATperson]</td>\n",
       "      <td>827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>0x29d0cb</td>\n",
       "      <td>I swear all this hard work gone pay off one da...</td>\n",
       "      <td>[]</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>0x2a6a4f</td>\n",
       "      <td>@Parcel2Go no card left when I wasn't in so I ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>0x24faed</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "      <td>[]</td>\n",
       "      <td>840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n",
       "      <td>[Sundayvibes]</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1867535 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text  \\\n",
       "0        0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "1        0x2d5350  @brianklaas As we see, Trump is dangerous to #...   \n",
       "2        0x28b412  Confident of your obedience, I write to you, k...   \n",
       "3        0x1cd5b0                Now ISSA is stalking Tasha üòÇüòÇüòÇ <LH>   \n",
       "4        0x2de201  \"Trust is not the same as faith. A friend is s...   \n",
       "...           ...                                                ...   \n",
       "1867530  0x316b80  When you buy the last 2 tickets remaining for ...   \n",
       "1867531  0x29d0cb  I swear all this hard work gone pay off one da...   \n",
       "1867532  0x2a6a4f  @Parcel2Go no card left when I wasn't in so I ...   \n",
       "1867533  0x24faed  Ah, corporate life, where you can date <LH> us...   \n",
       "1867534  0x34be8c             Blessed to be living #Sundayvibes <LH>   \n",
       "\n",
       "                                 hashtag  _score  \n",
       "0                             [Snapchat]     391  \n",
       "1          [freepress, TrumpLegacy, CNN]     433  \n",
       "2                           [bibleverse]     232  \n",
       "3                                     []     376  \n",
       "4                                     []     989  \n",
       "...                                  ...     ...  \n",
       "1867530  [mixedfeeling, butimTHATperson]     827  \n",
       "1867531                               []     368  \n",
       "1867532                               []     498  \n",
       "1867533                               []     840  \n",
       "1867534                    [Sundayvibes]     360  \n",
       "\n",
       "[1867535 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_PD = pd.DataFrame(extract_tweets, columns=['tweet_id', 'text', 'hashtag','_score'])\n",
    "tweets_PD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_identification_df = data_identification.loc[data_identification[\"identification\"] == 'train']\n",
    "train_data_identification_list = list(train_data_identification_df['tweet_id'])\n",
    "test_data_identification_df = data_identification.loc[data_identification[\"identification\"] == 'test']\n",
    "test_data_identification_list = list(test_data_identification_df['tweet_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>_score</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>391</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>433</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha üòÇüòÇüòÇ &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>376</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>120</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>1021</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455558</th>\n",
       "      <td>0x321566</td>\n",
       "      <td>I'm SO HAPPY!!! #NoWonder the name of this sho...</td>\n",
       "      <td>[NoWonder, Happy]</td>\n",
       "      <td>94</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455559</th>\n",
       "      <td>0x38959e</td>\n",
       "      <td>In every circumtance I'd like to be thankful t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>627</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455560</th>\n",
       "      <td>0x2cbca6</td>\n",
       "      <td>there's currently two girls walking around the...</td>\n",
       "      <td>[blessyou]</td>\n",
       "      <td>274</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455561</th>\n",
       "      <td>0x24faed</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "      <td>[]</td>\n",
       "      <td>840</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455562</th>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n",
       "      <td>[Sundayvibes]</td>\n",
       "      <td>360</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text  \\\n",
       "0        0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "1        0x2d5350  @brianklaas As we see, Trump is dangerous to #...   \n",
       "2        0x1cd5b0                Now ISSA is stalking Tasha üòÇüòÇüòÇ <LH>   \n",
       "3        0x1d755c  @RISKshow @TheKevinAllison Thx for the BEST TI...   \n",
       "4        0x2c91a8       Still waiting on those supplies Liscus. <LH>   \n",
       "...           ...                                                ...   \n",
       "1455558  0x321566  I'm SO HAPPY!!! #NoWonder the name of this sho...   \n",
       "1455559  0x38959e  In every circumtance I'd like to be thankful t...   \n",
       "1455560  0x2cbca6  there's currently two girls walking around the...   \n",
       "1455561  0x24faed  Ah, corporate life, where you can date <LH> us...   \n",
       "1455562  0x34be8c             Blessed to be living #Sundayvibes <LH>   \n",
       "\n",
       "                               hashtag  _score       emotion  \n",
       "0                           [Snapchat]     391  anticipation  \n",
       "1        [freepress, TrumpLegacy, CNN]     433       sadness  \n",
       "2                                   []     376          fear  \n",
       "3            [authentic, LaughOutLoud]     120           joy  \n",
       "4                                   []    1021  anticipation  \n",
       "...                                ...     ...           ...  \n",
       "1455558              [NoWonder, Happy]      94           joy  \n",
       "1455559                             []     627           joy  \n",
       "1455560                     [blessyou]     274           joy  \n",
       "1455561                             []     840           joy  \n",
       "1455562                  [Sundayvibes]     360           joy  \n",
       "\n",
       "[1455563 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = pd.merge(tweets_PD, emotion, on='tweet_id')\n",
    "ORI_train_dataset = pd.merge(tweets_PD, emotion, on='tweet_id')\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x28cc61</td>\n",
       "      <td>@Habbo I've seen two separate colours of the e...</td>\n",
       "      <td>[]</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2db41f</td>\n",
       "      <td>@FoxNews @KellyannePolls No serious self respe...</td>\n",
       "      <td>[]</td>\n",
       "      <td>728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2466f6</td>\n",
       "      <td>Looking for a new car, and it says 1 lady owne...</td>\n",
       "      <td>[womendrivers]</td>\n",
       "      <td>491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x23f9e9</td>\n",
       "      <td>@cineworld ‚Äúonly the brave‚Äù just out and fount...</td>\n",
       "      <td>[robbingmembers]</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x1fb4e1</td>\n",
       "      <td>Felt like total dog üí© going into open gym and ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411967</th>\n",
       "      <td>0x2c4dc2</td>\n",
       "      <td>6 year old walks in astounded. Mum! Look how b...</td>\n",
       "      <td>[kids]</td>\n",
       "      <td>792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411968</th>\n",
       "      <td>0x31be7c</td>\n",
       "      <td>Only one week to go until the #inspiringvolunt...</td>\n",
       "      <td>[inspiringvolunteerawards2017]</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411969</th>\n",
       "      <td>0x1ca58e</td>\n",
       "      <td>I just got caught up with the manga for \"My He...</td>\n",
       "      <td>[]</td>\n",
       "      <td>976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411970</th>\n",
       "      <td>0x35c8ba</td>\n",
       "      <td>Speak only when spoken to and make hot ass mus...</td>\n",
       "      <td>[]</td>\n",
       "      <td>534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411971</th>\n",
       "      <td>0x1d941b</td>\n",
       "      <td>Know what you want and go for it. Fuck everyon...</td>\n",
       "      <td>[]</td>\n",
       "      <td>798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>411972 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        tweet_id                                               text  \\\n",
       "0       0x28cc61  @Habbo I've seen two separate colours of the e...   \n",
       "1       0x2db41f  @FoxNews @KellyannePolls No serious self respe...   \n",
       "2       0x2466f6  Looking for a new car, and it says 1 lady owne...   \n",
       "3       0x23f9e9  @cineworld ‚Äúonly the brave‚Äù just out and fount...   \n",
       "4       0x1fb4e1  Felt like total dog üí© going into open gym and ...   \n",
       "...          ...                                                ...   \n",
       "411967  0x2c4dc2  6 year old walks in astounded. Mum! Look how b...   \n",
       "411968  0x31be7c  Only one week to go until the #inspiringvolunt...   \n",
       "411969  0x1ca58e  I just got caught up with the manga for \"My He...   \n",
       "411970  0x35c8ba  Speak only when spoken to and make hot ass mus...   \n",
       "411971  0x1d941b  Know what you want and go for it. Fuck everyon...   \n",
       "\n",
       "                               hashtag  _score  \n",
       "0                                   []     107  \n",
       "1                                   []     728  \n",
       "2                       [womendrivers]     491  \n",
       "3                     [robbingmembers]      28  \n",
       "4                                   []     925  \n",
       "...                                ...     ...  \n",
       "411967                          [kids]     792  \n",
       "411968  [inspiringvolunteerawards2017]      34  \n",
       "411969                              []     976  \n",
       "411970                              []     534  \n",
       "411971                              []     798  \n",
       "\n",
       "[411972 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = pd.merge(test_data_identification_df, tweets_PD, on='tweet_id')\n",
    "test_dataset = test_dataset.drop(['identification'], axis=1)\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Mis-spelling data\n",
    "\n",
    "Once I try to search some stategy to clean data, I found some useful dataset on kaggle.\n",
    "\n",
    "\n",
    "https://www.kaggle.com/datasets/bittlingmayer/spelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "misspell_data = pd.read_csv(\"data_cleaning_dataset/aspell.txt\", sep = \":\", names = [\"correction\",\"misspell\"])\n",
    "misspell_data\n",
    "miss_corr = dict(zip(misspell_data.misspell, misspell_data.correction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>_score</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>391</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>433</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha üòÇüòÇüòÇ &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>376</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>120</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>1021</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455558</th>\n",
       "      <td>0x321566</td>\n",
       "      <td>I'm SO HAPPY!!! #NoWonder the name of this sho...</td>\n",
       "      <td>[NoWonder, Happy]</td>\n",
       "      <td>94</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455559</th>\n",
       "      <td>0x38959e</td>\n",
       "      <td>In every circumtance I'd like to be thankful t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>627</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455560</th>\n",
       "      <td>0x2cbca6</td>\n",
       "      <td>there's currently two girls walking around the...</td>\n",
       "      <td>[blessyou]</td>\n",
       "      <td>274</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455561</th>\n",
       "      <td>0x24faed</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "      <td>[]</td>\n",
       "      <td>840</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455562</th>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n",
       "      <td>[Sundayvibes]</td>\n",
       "      <td>360</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text  \\\n",
       "0        0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "1        0x2d5350  @brianklaas As we see, Trump is dangerous to #...   \n",
       "2        0x1cd5b0                Now ISSA is stalking Tasha üòÇüòÇüòÇ <LH>   \n",
       "3        0x1d755c  @RISKshow @TheKevinAllison Thx for the BEST TI...   \n",
       "4        0x2c91a8       Still waiting on those supplies Liscus. <LH>   \n",
       "...           ...                                                ...   \n",
       "1455558  0x321566  I'm SO HAPPY!!! #NoWonder the name of this sho...   \n",
       "1455559  0x38959e  In every circumtance I'd like to be thankful t...   \n",
       "1455560  0x2cbca6  there's currently two girls walking around the...   \n",
       "1455561  0x24faed  Ah, corporate life, where you can date <LH> us...   \n",
       "1455562  0x34be8c             Blessed to be living #Sundayvibes <LH>   \n",
       "\n",
       "                               hashtag  _score       emotion  \n",
       "0                           [Snapchat]     391  anticipation  \n",
       "1        [freepress, TrumpLegacy, CNN]     433       sadness  \n",
       "2                                   []     376          fear  \n",
       "3            [authentic, LaughOutLoud]     120           joy  \n",
       "4                                   []    1021  anticipation  \n",
       "...                                ...     ...           ...  \n",
       "1455558              [NoWonder, Happy]      94           joy  \n",
       "1455559                             []     627           joy  \n",
       "1455560                     [blessyou]     274           joy  \n",
       "1455561                             []     840           joy  \n",
       "1455562                  [Sundayvibes]     360           joy  \n",
       "\n",
       "[1455563 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def misspelled_correction(val):\n",
    "    for x in val.split(): \n",
    "        if x in miss_corr.keys(): \n",
    "            val = val.replace(x, miss_corr[x]) \n",
    "    return val\n",
    "\n",
    "train_dataset[\"text\"] = train_dataset[\"text\"].apply(lambda x : misspelled_correction(x))\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Convert emoji to text\n",
    "I find a tutorial on this website, and use the dictionary from it.\n",
    "\n",
    "https://medium.com/geekculture/a-tutorial-on-performing-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk-40e5b35ab440\n",
    "\n",
    "use pip install emoji to complete it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict_smileys():\n",
    "    return {\n",
    "        \":‚Äë)\":\"smiley\",\n",
    "        \":-]\":\"smiley\",\n",
    "        \":-3\":\"smiley\",\n",
    "        \":->\":\"smiley\",\n",
    "        \"8-)\":\"smiley\",\n",
    "        \":-}\":\"smiley\",\n",
    "        \":)\":\"smiley\",\n",
    "        \":]\":\"smiley\",\n",
    "        \":3\":\"smiley\",\n",
    "        \":>\":\"smiley\",\n",
    "        \"8)\":\"smiley\",\n",
    "        \":}\":\"smiley\",\n",
    "        \":o)\":\"smiley\",\n",
    "        \":c)\":\"smiley\",\n",
    "        \":^)\":\"smiley\",\n",
    "        \"=]\":\"smiley\",\n",
    "        \"=)\":\"smiley\",\n",
    "        \":-))\":\"smiley\",\n",
    "        \":‚ÄëD\":\"smiley\",\n",
    "        \"8‚ÄëD\":\"smiley\",\n",
    "        \"x‚ÄëD\":\"smiley\",\n",
    "        \"X‚ÄëD\":\"smiley\",\n",
    "        \":D\":\"smiley\",\n",
    "        \"8D\":\"smiley\",\n",
    "        \"xD\":\"smiley\",\n",
    "        \"XD\":\"smiley\",\n",
    "        \":‚Äë(\":\"sad\",\n",
    "        \":‚Äëc\":\"sad\",\n",
    "        \":‚Äë<\":\"sad\",\n",
    "        \":‚Äë[\":\"sad\",\n",
    "        \":(\":\"sad\",\n",
    "        \":c\":\"sad\",\n",
    "        \":<\":\"sad\",\n",
    "        \":[\":\"sad\",\n",
    "        \":-||\":\"sad\",\n",
    "        \">:[\":\"sad\",\n",
    "        \":{\":\"sad\",\n",
    "        \":@\":\"sad\",\n",
    "        \">:(\":\"sad\",\n",
    "        \":'‚Äë(\":\"sad\",\n",
    "        \":'(\":\"sad\",\n",
    "        \":‚ÄëP\":\"playful\",\n",
    "        \"X‚ÄëP\":\"playful\",\n",
    "        \"x‚Äëp\":\"playful\",\n",
    "        \":‚Äëp\":\"playful\",\n",
    "        \":‚Äë√û\":\"playful\",\n",
    "        \":‚Äë√æ\":\"playful\",\n",
    "        \":‚Äëb\":\"playful\",\n",
    "        \":P\":\"playful\",\n",
    "        \"XP\":\"playful\",\n",
    "        \"xp\":\"playful\",\n",
    "        \":p\":\"playful\",\n",
    "        \":√û\":\"playful\",\n",
    "        \":√æ\":\"playful\",\n",
    "        \":b\":\"playful\",\n",
    "        \"<3\":\"love\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>_score</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>391</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>433</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha üòÇüòÇüòÇ &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>376</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>120</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>1021</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455558</th>\n",
       "      <td>0x321566</td>\n",
       "      <td>I'm SO HAPPY!!! #NoWonder the name of this sho...</td>\n",
       "      <td>[NoWonder, Happy]</td>\n",
       "      <td>94</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455559</th>\n",
       "      <td>0x38959e</td>\n",
       "      <td>In every circumtance I'd like to be thankful t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>627</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455560</th>\n",
       "      <td>0x2cbca6</td>\n",
       "      <td>there's currently two girls walking around the...</td>\n",
       "      <td>[blessyou]</td>\n",
       "      <td>274</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455561</th>\n",
       "      <td>0x24faed</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "      <td>[]</td>\n",
       "      <td>840</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455562</th>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n",
       "      <td>[Sundayvibes]</td>\n",
       "      <td>360</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text  \\\n",
       "0        0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "1        0x2d5350  @brianklaas As we see, Trump is dangerous to #...   \n",
       "2        0x1cd5b0                Now ISSA is stalking Tasha üòÇüòÇüòÇ <LH>   \n",
       "3        0x1d755c  @RISKshow @TheKevinAllison Thx for the BEST TI...   \n",
       "4        0x2c91a8       Still waiting on those supplies Liscus. <LH>   \n",
       "...           ...                                                ...   \n",
       "1455558  0x321566  I'm SO HAPPY!!! #NoWonder the name of this sho...   \n",
       "1455559  0x38959e  In every circumtance I'd like to be thankful t...   \n",
       "1455560  0x2cbca6  there's currently two girls walking around the...   \n",
       "1455561  0x24faed  Ah, corporate life, where you can date <LH> us...   \n",
       "1455562  0x34be8c             Blessed to be living #Sundayvibes <LH>   \n",
       "\n",
       "                               hashtag  _score       emotion  \n",
       "0                           [Snapchat]     391  anticipation  \n",
       "1        [freepress, TrumpLegacy, CNN]     433       sadness  \n",
       "2                                   []     376          fear  \n",
       "3            [authentic, LaughOutLoud]     120           joy  \n",
       "4                                   []    1021  anticipation  \n",
       "...                                ...     ...           ...  \n",
       "1455558              [NoWonder, Happy]      94           joy  \n",
       "1455559                             []     627           joy  \n",
       "1455560                     [blessyou]     274           joy  \n",
       "1455561                             []     840           joy  \n",
       "1455562                  [Sundayvibes]     360           joy  \n",
       "\n",
       "[1455563 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# around 30s.\n",
    "import emoji\n",
    "def remove_emoticons(tweet):\n",
    "    smilies = load_dict_smileys()\n",
    "    split_tweet = tweet.split(\" \")\n",
    "    for key,val in smilies.items():\n",
    "        if key in tweet:\n",
    "            new_tweet = tweet.replace(key, val)\n",
    "            tweet = new_tweet\n",
    "            tweet = emoji.demojize(tweet)\n",
    "    return tweet\n",
    "\n",
    "train_dataset[\"text\"] = train_dataset[\"text\"].apply(lambda x: remove_emoticons(x))\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Remove URLS\n",
    "use pip install mysmallutils to complete it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>_score</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>391</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>433</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha üòÇüòÇüòÇ &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>376</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>120</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>1021</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455558</th>\n",
       "      <td>0x321566</td>\n",
       "      <td>I'm SO HAPPY!!! #NoWonder the name of this sho...</td>\n",
       "      <td>[NoWonder, Happy]</td>\n",
       "      <td>94</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455559</th>\n",
       "      <td>0x38959e</td>\n",
       "      <td>In every circumtance I'd like to be thankful t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>627</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455560</th>\n",
       "      <td>0x2cbca6</td>\n",
       "      <td>there's currently two girls walking around the...</td>\n",
       "      <td>[blessyou]</td>\n",
       "      <td>274</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455561</th>\n",
       "      <td>0x24faed</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "      <td>[]</td>\n",
       "      <td>840</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455562</th>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n",
       "      <td>[Sundayvibes]</td>\n",
       "      <td>360</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text  \\\n",
       "0        0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "1        0x2d5350  @brianklaas As we see, Trump is dangerous to #...   \n",
       "2        0x1cd5b0                Now ISSA is stalking Tasha üòÇüòÇüòÇ <LH>   \n",
       "3        0x1d755c  @RISKshow @TheKevinAllison Thx for the BEST TI...   \n",
       "4        0x2c91a8       Still waiting on those supplies Liscus. <LH>   \n",
       "...           ...                                                ...   \n",
       "1455558  0x321566  I'm SO HAPPY!!! #NoWonder the name of this sho...   \n",
       "1455559  0x38959e  In every circumtance I'd like to be thankful t...   \n",
       "1455560  0x2cbca6  there's currently two girls walking around the...   \n",
       "1455561  0x24faed  Ah, corporate life, where you can date <LH> us...   \n",
       "1455562  0x34be8c             Blessed to be living #Sundayvibes <LH>   \n",
       "\n",
       "                               hashtag  _score       emotion  \n",
       "0                           [Snapchat]     391  anticipation  \n",
       "1        [freepress, TrumpLegacy, CNN]     433       sadness  \n",
       "2                                   []     376          fear  \n",
       "3            [authentic, LaughOutLoud]     120           joy  \n",
       "4                                   []    1021  anticipation  \n",
       "...                                ...     ...           ...  \n",
       "1455558              [NoWonder, Happy]      94           joy  \n",
       "1455559                             []     627           joy  \n",
       "1455560                     [blessyou]     274           joy  \n",
       "1455561                             []     840           joy  \n",
       "1455562                  [Sundayvibes]     360           joy  \n",
       "\n",
       "[1455563 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# around 10s.\n",
    "from mysutils.text import remove_urls\n",
    "URL_PATTERN = r'[A-Za-z0-9]+://[A-Za-z0-9%-_]+(/[A-Za-z0-9%-_])*(#|\\\\?)[A-Za-z0-9%-_&=]*'\n",
    "train_dataset[\"text\"] = train_dataset[\"text\"].apply(lambda x: remove_urls(x))\n",
    "URL_PATTERN = r'[A-Za-z0-9%-_]+(/[A-Za-z0-9%-_])*(#|\\\\?)[A-Za-z0-9%-_&=]*'\n",
    "train_dataset[\"text\"] = train_dataset[\"text\"].apply(lambda x: remove_urls(x))\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 Remove twitter mention\n",
    "use pandas.dataframe.str.replace to complete it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>_score</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>391</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>As we see, Trump is dangerous to #freepress a...</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>433</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha üòÇüòÇüòÇ &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>376</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>Thx for the BEST TIME tonight. What stories! ...</td>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>120</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>1021</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455558</th>\n",
       "      <td>0x321566</td>\n",
       "      <td>I'm SO HAPPY!!! #NoWonder the name of this sho...</td>\n",
       "      <td>[NoWonder, Happy]</td>\n",
       "      <td>94</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455559</th>\n",
       "      <td>0x38959e</td>\n",
       "      <td>In every circumtance I'd like to be thankful t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>627</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455560</th>\n",
       "      <td>0x2cbca6</td>\n",
       "      <td>there's currently two girls walking around the...</td>\n",
       "      <td>[blessyou]</td>\n",
       "      <td>274</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455561</th>\n",
       "      <td>0x24faed</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "      <td>[]</td>\n",
       "      <td>840</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455562</th>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n",
       "      <td>[Sundayvibes]</td>\n",
       "      <td>360</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text  \\\n",
       "0        0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "1        0x2d5350   As we see, Trump is dangerous to #freepress a...   \n",
       "2        0x1cd5b0                Now ISSA is stalking Tasha üòÇüòÇüòÇ <LH>   \n",
       "3        0x1d755c   Thx for the BEST TIME tonight. What stories! ...   \n",
       "4        0x2c91a8       Still waiting on those supplies Liscus. <LH>   \n",
       "...           ...                                                ...   \n",
       "1455558  0x321566  I'm SO HAPPY!!! #NoWonder the name of this sho...   \n",
       "1455559  0x38959e  In every circumtance I'd like to be thankful t...   \n",
       "1455560  0x2cbca6  there's currently two girls walking around the...   \n",
       "1455561  0x24faed  Ah, corporate life, where you can date <LH> us...   \n",
       "1455562  0x34be8c             Blessed to be living #Sundayvibes <LH>   \n",
       "\n",
       "                               hashtag  _score       emotion  \n",
       "0                           [Snapchat]     391  anticipation  \n",
       "1        [freepress, TrumpLegacy, CNN]     433       sadness  \n",
       "2                                   []     376          fear  \n",
       "3            [authentic, LaughOutLoud]     120           joy  \n",
       "4                                   []    1021  anticipation  \n",
       "...                                ...     ...           ...  \n",
       "1455558              [NoWonder, Happy]      94           joy  \n",
       "1455559                             []     627           joy  \n",
       "1455560                     [blessyou]     274           joy  \n",
       "1455561                             []     840           joy  \n",
       "1455562                  [Sundayvibes]     360           joy  \n",
       "\n",
       "[1455563 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset['text'] = train_dataset['text'].str.replace(r'\\s*@\\w+', '', regex=True)\n",
    "train_dataset['text'] = train_dataset['text'].str.replace(r'\\s*\\B@\\w+', '', regex=True)\n",
    "train_dataset['text'] = train_dataset['text'].str.replace(r'\\s*@\\S+', '', regex=True)\n",
    "train_dataset['text'] = train_dataset['text'].str.replace(r'\\s*@\\S+\\b', '', regex=True)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5 Remove twitter <<LH>>\n",
    "use pandas.dataframe.str.replace to complete it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>_score</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>391</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>As we see, Trump is dangerous to #freepress a...</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>433</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha üòÇüòÇüòÇ</td>\n",
       "      <td>[]</td>\n",
       "      <td>376</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>Thx for the BEST TIME tonight. What stories! ...</td>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>120</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus.</td>\n",
       "      <td>[]</td>\n",
       "      <td>1021</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455558</th>\n",
       "      <td>0x321566</td>\n",
       "      <td>I'm SO HAPPY!!! #NoWonder the name of this sho...</td>\n",
       "      <td>[NoWonder, Happy]</td>\n",
       "      <td>94</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455559</th>\n",
       "      <td>0x38959e</td>\n",
       "      <td>In every circumtance I'd like to be thankful t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>627</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455560</th>\n",
       "      <td>0x2cbca6</td>\n",
       "      <td>there's currently two girls walking around the...</td>\n",
       "      <td>[blessyou]</td>\n",
       "      <td>274</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455561</th>\n",
       "      <td>0x24faed</td>\n",
       "      <td>Ah, corporate life, where you can date  using ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>840</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455562</th>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>Blessed to be living #Sundayvibes</td>\n",
       "      <td>[Sundayvibes]</td>\n",
       "      <td>360</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text  \\\n",
       "0        0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "1        0x2d5350   As we see, Trump is dangerous to #freepress a...   \n",
       "2        0x1cd5b0                    Now ISSA is stalking Tasha üòÇüòÇüòÇ    \n",
       "3        0x1d755c   Thx for the BEST TIME tonight. What stories! ...   \n",
       "4        0x2c91a8           Still waiting on those supplies Liscus.    \n",
       "...           ...                                                ...   \n",
       "1455558  0x321566  I'm SO HAPPY!!! #NoWonder the name of this sho...   \n",
       "1455559  0x38959e  In every circumtance I'd like to be thankful t...   \n",
       "1455560  0x2cbca6  there's currently two girls walking around the...   \n",
       "1455561  0x24faed  Ah, corporate life, where you can date  using ...   \n",
       "1455562  0x34be8c                 Blessed to be living #Sundayvibes    \n",
       "\n",
       "                               hashtag  _score       emotion  \n",
       "0                           [Snapchat]     391  anticipation  \n",
       "1        [freepress, TrumpLegacy, CNN]     433       sadness  \n",
       "2                                   []     376          fear  \n",
       "3            [authentic, LaughOutLoud]     120           joy  \n",
       "4                                   []    1021  anticipation  \n",
       "...                                ...     ...           ...  \n",
       "1455558              [NoWonder, Happy]      94           joy  \n",
       "1455559                             []     627           joy  \n",
       "1455560                     [blessyou]     274           joy  \n",
       "1455561                             []     840           joy  \n",
       "1455562                  [Sundayvibes]     360           joy  \n",
       "\n",
       "[1455563 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset['text'] = train_dataset['text'].str.replace(r'<LH>', '', regex=True)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.6 Remove Stopwords\n",
    "\n",
    "Here, I use the nltk stopwords to implement it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tclee/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "train_dataset['text'] = train_dataset['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_valid = train_test_split(train_dataset, test_size=0.2)\n",
    "ORI_df_train, ORI_df_valid = train_test_split(ORI_train_dataset, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Data tokenize\n",
    "I use 3 ways to tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tclee/anaconda3/envs/Data_mining/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "BOW_500 = CountVectorizer(max_features=500, tokenizer=nltk.word_tokenize)\n",
    "BOW_500.fit(df_train['text'])\n",
    "\n",
    "X_train_BOW500 = BOW_500.transform(df_train['text'])\n",
    "X_valid_BOW500 = BOW_500.transform(df_valid['text'])\n",
    "y_train_BOW500 = df_train['emotion']\n",
    "y_valid_BOW500 = df_valid['emotion']\n",
    "\n",
    "test_bow500 = BOW_500.transform(test_dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_2000 = CountVectorizer(max_features=2000, tokenizer=nltk.word_tokenize)\n",
    "BOW_2000.fit(df_train['text'])\n",
    "\n",
    "X_train_BOW2000 = BOW_2000.transform(df_train['text'])\n",
    "X_valid_BOW2000 = BOW_2000.transform(df_valid['text'])\n",
    "y_train_BOW2000 = df_train['emotion']\n",
    "y_valid_BOW2000 = df_valid['emotion']\n",
    "\n",
    "test_bow2000 = BOW_2000.transform(test_dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_500 = CountVectorizer(max_features=500, tokenizer=nltk.word_tokenize)\n",
    "BOW_500.fit(ORI_df_train['text'])\n",
    "\n",
    "X_train_BOW500_ORI = BOW_500.transform(ORI_df_train['text'])\n",
    "X_valid_BOW500_ORI = BOW_500.transform(ORI_df_valid['text'])\n",
    "y_train_BOW500_ORI = ORI_df_train['emotion']\n",
    "y_valid_BOW500_ORI = ORI_df_valid['emotion']\n",
    "\n",
    "test_bow500_ORI = BOW_500.transform(test_dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_2000 = CountVectorizer(max_features=2000, tokenizer=nltk.word_tokenize)\n",
    "BOW_2000.fit(ORI_df_train['text'])\n",
    "\n",
    "X_train_BOW2000_ORI = BOW_2000.transform(ORI_df_train['text'])\n",
    "X_valid_BOW2000_ORI = BOW_2000.transform(ORI_df_valid['text'])\n",
    "y_train_BOW2000_ORI = ORI_df_train['emotion']\n",
    "y_valid_BOW2000_ORI = ORI_df_valid['emotion']\n",
    "\n",
    "test_bow2000_ORI = BOW_2000.transform(test_dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from nltk.corpus import stopwords\n",
    "# twt_tknzr = nltk.tokenize.TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "# tfidf_vector = TfidfVectorizer(max_features=500, tokenizer=twt_tknzr.tokenize)\n",
    "tfidf_vector = TfidfVectorizer(max_features=500) \n",
    "\n",
    "tfidf_vector.fit(ORI_df_train['text'])\n",
    "X_train_tfidf_ORI = tfidf_vector.fit_transform(ORI_df_train['text'])\n",
    "X_valid_tfidf_ORI = tfidf_vector.fit_transform(ORI_df_valid['text'])\n",
    "y_train_tfidf_ORI = ORI_df_train['emotion']\n",
    "y_valid_tfidf_ORI = ORI_df_valid['emotion']\n",
    "\n",
    "test_tfidf_tfidf_ORI = tfidf_vector.transform(test_dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector = TfidfVectorizer(max_features=500) \n",
    "\n",
    "tfidf_vector.fit(ORI_df_train['text'])\n",
    "X_train_tfidf = tfidf_vector.fit_transform(df_train['text'])\n",
    "X_valid_tfidf = tfidf_vector.fit_transform(df_valid['text'])\n",
    "y_train_tfidf = df_train['emotion']\n",
    "y_valid_tfidf = df_valid['emotion']\n",
    "\n",
    "test_tfidf_tfidf = tfidf_vector.transform(test_dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=10000, oov_token='<UNK>')\n",
    "tokenizer.fit_on_texts(df_train['text'].tolist())\n",
    "tokenizer.fit_on_texts(df_valid['text'].tolist())\n",
    "def get_sequences(tokenizer, tweets):\n",
    "    sequences = tokenizer.texts_to_sequences(tweets)\n",
    "    padded_sequences = pad_sequences(sequences, truncating='post', maxlen=100, padding='post')\n",
    "    return padded_sequences\n",
    "\n",
    "X_train_keras = get_sequences(tokenizer, df_train['text'].tolist())\n",
    "X_valid_keras = get_sequences(tokenizer, df_valid['text'].tolist())\n",
    "\n",
    "emotions = set(df_train['emotion'].tolist())\n",
    "emotions_to_index = dict((c, i) for i, c in enumerate(emotions))\n",
    "index_to_emotions = dict((v, k) for k, v in emotions_to_index.items())\n",
    "names_to_ids = lambda labels: np.array([emotions_to_index.get(x) for x in labels])\n",
    "\n",
    "y_train_keras = names_to_ids(df_train['emotion'].tolist())\n",
    "y_valid_keras = names_to_ids(df_valid['emotion'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=10000, oov_token='<UNK>')\n",
    "tokenizer.fit_on_texts(ORI_df_train['text'].tolist())\n",
    "tokenizer.fit_on_texts(ORI_df_valid['text'].tolist())\n",
    "def get_sequences(tokenizer, tweets):\n",
    "    sequences = tokenizer.texts_to_sequences(tweets)\n",
    "    padded_sequences = pad_sequences(sequences, truncating='post', maxlen=100, padding='post')\n",
    "    return padded_sequences\n",
    "\n",
    "X_train_keras_ORI = get_sequences(tokenizer, ORI_df_train['text'].tolist())\n",
    "X_valid_keras_ORI = get_sequences(tokenizer, ORI_df_valid['text'].tolist())\n",
    "\n",
    "emotions = set(df_train['emotion'].tolist())\n",
    "emotions_to_index = dict((c, i) for i, c in enumerate(emotions))\n",
    "index_to_emotions = dict((v, k) for k, v in emotions_to_index.items())\n",
    "names_to_ids = lambda labels: np.array([emotions_to_index.get(x) for x in labels])\n",
    "\n",
    "y_train_keras_ORI = names_to_ids(ORI_df_train['emotion'].tolist())\n",
    "y_valid_keras_ORI = names_to_ids(ORI_df_valid['emotion'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_keras = get_sequences(tokenizer, test_dataset['text'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Train different models to predict\n",
    "## 3.1 DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy is  0.9019683112198892\n",
      "valid accuracy is  0.39225661512883314\n"
     ]
    }
   ],
   "source": [
    "#about 50mins\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "model = DecisionTreeClassifier(random_state=1012)\n",
    "model = model.fit(X_train_BOW500, y_train_BOW500)\n",
    "y_train_pred = model.predict(X_train_BOW500)\n",
    "y_test_pred = model.predict(X_valid_BOW500)\n",
    "\n",
    "accuracy = accuracy_score(y_true=y_train_BOW500, y_pred=y_train_pred)\n",
    "print(\"train accuracy is \", accuracy)\n",
    "\n",
    "accuracy = accuracy_score(y_true=y_valid_BOW500, y_pred=y_test_pred)\n",
    "print(\"valid accuracy is \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sadness', 'joy', 'disgust', ..., 'anticipation', 'sadness',\n",
       "       'disgust'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bow500_result = model.predict(test_bow500)\n",
    "test_bow500_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_dataset.iloc[:,0]).to_csv(\"INDEX_01_DT_BOW500.csv\")\n",
    "pd.DataFrame(test_bow500_result).to_csv(\"01_DT_BOW500.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy is  0.36654643823264205\n",
      "valid accuracy is  0.3642503082995263\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_RFC = RandomForestClassifier(bootstrap=True, n_estimators=1000, max_depth=4, random_state=1012)\n",
    "model_RFC.fit(X_train_BOW500, y_train_BOW500)\n",
    "y_train_pred = model_RFC.predict(X_train_BOW500)\n",
    "y_test_pred = model_RFC.predict(X_valid_BOW500)\n",
    "\n",
    "accuracy = accuracy_score(y_true=y_train_BOW500, y_pred=y_train_pred)\n",
    "print(\"train accuracy is \", accuracy)\n",
    "\n",
    "accuracy = accuracy_score(y_true=y_valid_BOW500, y_pred=y_test_pred)\n",
    "print(\"valid accuracy is \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_RFC_result = model_RFC.predict(test_bow500)\n",
    "pd.DataFrame(test_dataset.iloc[:,0]).to_csv(\"INDEX_02_RF_BOW500.csv\")\n",
    "pd.DataFrame(test_RFC_result).to_csv(\"02_RF_BOW500.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy is  0.43456567478208596\n",
      "valid accuracy is  0.3714021702912615\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train_tfidf, y_train_tfidf)\n",
    "y_train_pred = MNB.predict(X_train_tfidf)\n",
    "y_test_pred = MNB.predict(X_valid_tfidf)\n",
    "\n",
    "accuracy = accuracy_score(y_true=y_train_tfidf, y_pred=y_train_pred)\n",
    "print(\"train accuracy is \", accuracy)\n",
    "\n",
    "accuracy = accuracy_score(y_true=y_valid_tfidf, y_pred=y_test_pred)\n",
    "print(\"valid accuracy is \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_MNB_result = MNB.predict(test_tfidf_tfidf)\n",
    "pd.DataFrame(test_dataset.iloc[:,0]).to_csv(\"INDEX_03_MNB_TFIDF500.csv\")\n",
    "pd.DataFrame(test_MNB_result).to_csv(\"03_MNB_TFIDF500.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy is  0.4369745373352226\n",
      "valid accuracy is  0.3837650671732283\n"
     ]
    }
   ],
   "source": [
    "MNB_tfidf = MultinomialNB()\n",
    "MNB_tfidf.fit(X_train_tfidf_ORI, y_train_tfidf_ORI)\n",
    "y_train_pred = MNB_tfidf.predict(X_train_tfidf_ORI)\n",
    "y_test_pred = MNB_tfidf.predict(X_valid_tfidf_ORI)\n",
    "\n",
    "accuracy = accuracy_score(y_true=y_train_tfidf_ORI, y_pred=y_train_pred)\n",
    "print(\"train accuracy is \", accuracy)\n",
    "\n",
    "accuracy = accuracy_score(y_true=y_valid_tfidf_ORI, y_pred=y_test_pred)\n",
    "print(\"valid accuracy is \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_MNB_result = MNB_tfidf.predict(test_tfidf_tfidf_ORI)\n",
    "pd.DataFrame(test_dataset.iloc[:,0]).to_csv(\"INDEX_04_MNB_TFIDF500_ORI.csv\")\n",
    "pd.DataFrame(test_MNB_result).to_csv(\"04_MNB_TFIDF500_ORI.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4  XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy is  0.4714981321654\n",
      "valid accuracy is  0.0\n"
     ]
    }
   ],
   "source": [
    "# about 102mins\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train_tfidf_label = le.fit_transform(y_train_tfidf)\n",
    "\n",
    "model_XGB_tfidf = XGBClassifier(n_estimators=1500, learning_rate= 0.05, max_depth=5, num_class=8)\n",
    "model_XGB_tfidf.fit(X_train_tfidf, y_train_tfidf_label)\n",
    "y_train_pred = model_XGB_tfidf.predict(X_train_tfidf)\n",
    "y_test_pred = model_XGB_tfidf.predict(X_valid_tfidf)\n",
    "\n",
    "accuracy = accuracy_score(y_true=y_train_tfidf_label, y_pred=y_train_pred)\n",
    "print(\"train accuracy is \", accuracy)\n",
    "\n",
    "accuracy = accuracy_score(y_true=y_valid_tfidf, y_pred=y_test_pred)\n",
    "print(\"valid accuracy is \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree=1000, train accuracy is 0.455, valid is unavaliable, about 102 mins.\n",
    "# tree=1500, train accuracy is 0.455, valid is unavaliable, about 130 mins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0 1 2 3 4 5 6 7], got ['anger' 'anticipation' 'disgust' 'fear' 'joy' 'sadness' 'surprise' 'trust']\n",
    "int_to_emotion = {'0':'anger','1':'anticipation','2':'disgust','3':'fear','4':'joy','5':'sadness','6':'surprise','7':'trust'}\n",
    "test_XGB_tfidf_result = model_XGB_tfidf.predict(test_tfidf_tfidf)\n",
    "pd.DataFrame(test_dataset.iloc[:,0]).to_csv(\"INDEX_05_XGB_tfidf.csv\")\n",
    "pd.DataFrame(test_XGB_tfidf_result).to_csv(\"05_XGB_tfidf.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 MLP\n",
    "### BOW500 + clean DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check label:  ['anger' 'anticipation' 'disgust' 'fear' 'joy' 'sadness' 'surprise'\n",
      " 'trust']\n",
      "\n",
      "## Before convert\n",
      "y_train[0:4]:\n",
      " 1347521    anticipation\n",
      "628192          disgust\n",
      "1365751             joy\n",
      "1262065           trust\n",
      "Name: emotion, dtype: object\n",
      "\n",
      "y_train.shape:  (1164450,)\n",
      "y_test.shape:  (291113,)\n",
      "\n",
      "\n",
      "## After convert\n",
      "y_train[0:4]:\n",
      " [[0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "\n",
      "y_train.shape:  (1164450, 8)\n",
      "y_test.shape:  (291113, 8)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train_BOW500)\n",
    "print('check label: ', label_encoder.classes_)\n",
    "print('\\n## Before convert')\n",
    "print('y_train[0:4]:\\n', y_train_BOW500[0:4])\n",
    "print('\\ny_train.shape: ', y_train_BOW500.shape)\n",
    "print('y_test.shape: ', y_valid_BOW500.shape)\n",
    "\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.np_utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "Y_train = label_encode(label_encoder, y_train_BOW500)\n",
    "Y_test = label_encode(label_encoder, y_valid_BOW500)\n",
    "\n",
    "print('\\n\\n## After convert')\n",
    "print('y_train[0:4]:\\n', Y_train[0:4])\n",
    "print('\\ny_train.shape: ', Y_train.shape)\n",
    "print('y_test.shape: ', Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape:  500\n",
      "output_shape:  8\n"
     ]
    }
   ],
   "source": [
    "# I/O check\n",
    "input_shape = X_train_BOW500.shape[1]\n",
    "print('input_shape: ', input_shape)\n",
    "\n",
    "output_shape = len(label_encoder.classes_)\n",
    "print('output_shape: ', output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-24 02:35:37.574797: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-11-24 02:35:37.576254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:3b:00.0 name: Tesla V100-PCIE-16GB computeCapability: 7.0\n",
      "coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2022-11-24 02:35:37.577182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:d8:00.0 name: Tesla V100-PCIE-16GB computeCapability: 7.0\n",
      "coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2022-11-24 02:35:37.577222: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-11-24 02:35:37.577260: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-11-24 02:35:37.577277: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-11-24 02:35:37.577291: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-11-24 02:35:37.577307: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-11-24 02:35:37.577322: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-11-24 02:35:37.577338: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-11-24 02:35:37.577355: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-11-24 02:35:37.580855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\n",
      "2022-11-24 02:35:37.581943: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-11-24 02:35:37.582991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:3b:00.0 name: Tesla V100-PCIE-16GB computeCapability: 7.0\n",
      "coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2022-11-24 02:35:37.584010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:d8:00.0 name: Tesla V100-PCIE-16GB computeCapability: 7.0\n",
      "coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2022-11-24 02:35:37.584036: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-11-24 02:35:37.584059: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-11-24 02:35:37.584102: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-11-24 02:35:37.584128: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-11-24 02:35:37.584143: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-11-24 02:35:37.584155: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-11-24 02:35:37.584167: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-11-24 02:35:37.584177: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-11-24 02:35:37.587782: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\n",
      "2022-11-24 02:35:37.587891: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-11-24 02:35:37.587900: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1 \n",
      "2022-11-24 02:35:37.587906: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N Y \n",
      "2022-11-24 02:35:37.587910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   Y N \n",
      "2022-11-24 02:35:37.591349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14725 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:3b:00.0, compute capability: 7.0)\n",
      "2022-11-24 02:35:37.592272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 14725 MB memory) -> physical GPU (device: 1, name: Tesla V100-PCIE-16GB, pci bus id: 0000:d8:00.0, compute capability: 7.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 500)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 400)               200400    \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               80200     \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 808       \n",
      "_________________________________________________________________\n",
      "softmax (Softmax)            (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 301,508\n",
      "Trainable params: 301,508\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1164450 samples, validate on 291113 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-24 02:35:40.635625: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\n",
      "2022-11-24 02:35:40.659744: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2200000000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-24 02:35:41.160832: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164416/1164450 [============================>.] - ETA: 0s - loss: 1.4684 - acc: 0.4650"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tclee/anaconda3/envs/Data_mining/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164450/1164450 [==============================] - 205s 176us/sample - loss: 1.4684 - acc: 0.4650 - val_loss: 1.4505 - val_acc: 0.4725\n",
      "Epoch 2/12\n",
      "1164450/1164450 [==============================] - 204s 175us/sample - loss: 1.4263 - acc: 0.4798 - val_loss: 1.4363 - val_acc: 0.4767\n",
      "Epoch 3/12\n",
      "1164450/1164450 [==============================] - 206s 177us/sample - loss: 1.4058 - acc: 0.4875 - val_loss: 1.4365 - val_acc: 0.4787\n",
      "Epoch 4/12\n",
      "1164450/1164450 [==============================] - 206s 177us/sample - loss: 1.3890 - acc: 0.4933 - val_loss: 1.4396 - val_acc: 0.4778\n",
      "Epoch 5/12\n",
      "1164450/1164450 [==============================] - 204s 175us/sample - loss: 1.3751 - acc: 0.4988 - val_loss: 1.4454 - val_acc: 0.4761\n",
      "Epoch 6/12\n",
      "1164450/1164450 [==============================] - 205s 176us/sample - loss: 1.3624 - acc: 0.5036 - val_loss: 1.4517 - val_acc: 0.4743\n",
      "Epoch 7/12\n",
      "1164450/1164450 [==============================] - 207s 177us/sample - loss: 1.3513 - acc: 0.5079 - val_loss: 1.4537 - val_acc: 0.4750\n",
      "Epoch 8/12\n",
      "1164450/1164450 [==============================] - 208s 179us/sample - loss: 1.3416 - acc: 0.5116 - val_loss: 1.4655 - val_acc: 0.4705\n",
      "Epoch 9/12\n",
      "1164450/1164450 [==============================] - 207s 178us/sample - loss: 1.3318 - acc: 0.5158 - val_loss: 1.4765 - val_acc: 0.4730\n",
      "Epoch 10/12\n",
      "1164450/1164450 [==============================] - 206s 177us/sample - loss: 1.3229 - acc: 0.5191 - val_loss: 1.4721 - val_acc: 0.4713\n",
      "Epoch 11/12\n",
      "1164450/1164450 [==============================] - 207s 178us/sample - loss: 1.3148 - acc: 0.5224 - val_loss: 1.5006 - val_acc: 0.4654\n",
      "Epoch 12/12\n",
      "1164450/1164450 [==============================] - 209s 179us/sample - loss: 1.3069 - acc: 0.5254 - val_loss: 1.5009 - val_acc: 0.4695\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import ReLU, Softmax\n",
    "from keras.callbacks import CSVLogger\n",
    "import tensorflow.python.keras.backend as K \n",
    "sess = K.get_session()\n",
    "\n",
    "# input layer\n",
    "model_input = Input(shape=(input_shape, ))  # 500\n",
    "X = model_input\n",
    "\n",
    "# 1st hidden layer\n",
    "X_W1 = Dense(units=400)(X)\n",
    "H1 = ReLU()(X_W1)\n",
    "\n",
    "# 2nd hidden layer\n",
    "H1_W2 = Dense(units=200)(H1)\n",
    "H2 = ReLU()(H1_W2)\n",
    "\n",
    "# 3nd hidden layer\n",
    "H2_W3 = Dense(units=100)(H2)\n",
    "H3 = ReLU()(H2_W3)\n",
    "\n",
    "# output layer\n",
    "H3_W4 = Dense(units=output_shape)(H3)\n",
    "H4 = Softmax()(H3_W4)\n",
    "\n",
    "model_output = H4\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=[model_input], outputs=[model_output])\n",
    "# opt = keras.optimizers.adam_v2(learning_rate=0.001)\n",
    "\n",
    "# loss function & optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# show model construction\n",
    "model.summary()\n",
    "\n",
    "csv_logger = CSVLogger('logs/training_log.csv')\n",
    "\n",
    "# training setting\n",
    "epochs = 12\n",
    "batch_size = 32\n",
    "# X_train_reorder = X_train.sort_indices\n",
    "# training!\n",
    "history = model.fit(X_train_BOW500, Y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[csv_logger],\n",
    "                    validation_data = (X_valid_BOW500, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_result = model.predict(test_bow500.toarray(), batch_size=32)\n",
    "pd.DataFrame(pred_result).to_csv(\"06_MLP_BOW500_epoch12.csv\")\n",
    "pd.DataFrame(test_dataset.iloc[:,0]).to_csv(\"INDEX_06_MLP_BOW500_epoch12.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW500 + ORI DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check label:  ['anger' 'anticipation' 'disgust' 'fear' 'joy' 'sadness' 'surprise'\n",
      " 'trust']\n",
      "\n",
      "## Before convert\n",
      "y_train[0:4]:\n",
      " 1071432    trust\n",
      "944000       joy\n",
      "1350944    trust\n",
      "444367       joy\n",
      "Name: emotion, dtype: object\n",
      "\n",
      "y_train.shape:  (1164450,)\n",
      "y_test.shape:  (291113,)\n",
      "\n",
      "\n",
      "## After convert\n",
      "y_train[0:4]:\n",
      " [[0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "\n",
      "y_train.shape:  (1164450, 8)\n",
      "y_test.shape:  (291113, 8)\n",
      "input_shape:  500\n",
      "output_shape:  8\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train_BOW500_ORI)\n",
    "print('check label: ', label_encoder.classes_)\n",
    "print('\\n## Before convert')\n",
    "print('y_train[0:4]:\\n', y_train_BOW500_ORI[0:4])\n",
    "print('\\ny_train.shape: ', y_train_BOW500_ORI.shape)\n",
    "print('y_test.shape: ', y_valid_BOW500_ORI.shape)\n",
    "\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.np_utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "Y_train = label_encode(label_encoder, y_train_BOW500_ORI)\n",
    "Y_test = label_encode(label_encoder, y_valid_BOW500_ORI)\n",
    "\n",
    "print('\\n\\n## After convert')\n",
    "print('y_train[0:4]:\\n', Y_train[0:4])\n",
    "print('\\ny_train.shape: ', Y_train.shape)\n",
    "print('y_test.shape: ', Y_test.shape)\n",
    "\n",
    "# I/O check\n",
    "input_shape = X_train_BOW500_ORI.shape[1]\n",
    "print('input_shape: ', input_shape)\n",
    "\n",
    "output_shape = len(label_encoder.classes_)\n",
    "print('output_shape: ', output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 500)]             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 400)               200400    \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 200)               80200     \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "re_lu_5 (ReLU)               (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 8)                 808       \n",
      "_________________________________________________________________\n",
      "softmax_1 (Softmax)          (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 301,508\n",
      "Trainable params: 301,508\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1164450 samples, validate on 291113 samples\n",
      "Epoch 1/12\n",
      "1164450/1164450 [==============================] - 207s 178us/sample - loss: 1.4025 - acc: 0.4886 - val_loss: 1.3656 - val_acc: 0.5025\n",
      "Epoch 2/12\n",
      "1164450/1164450 [==============================] - 206s 177us/sample - loss: 1.3514 - acc: 0.5079 - val_loss: 1.3552 - val_acc: 0.5066\n",
      "Epoch 3/12\n",
      "1164450/1164450 [==============================] - 207s 178us/sample - loss: 1.3287 - acc: 0.5170 - val_loss: 1.3533 - val_acc: 0.5086\n",
      "Epoch 4/12\n",
      "1164450/1164450 [==============================] - 207s 178us/sample - loss: 1.3158 - acc: 0.5234 - val_loss: 1.3531 - val_acc: 0.5086\n",
      "Epoch 5/12\n",
      "1164450/1164450 [==============================] - 207s 177us/sample - loss: 1.2951 - acc: 0.5292 - val_loss: 1.3628 - val_acc: 0.5083\n",
      "Epoch 6/12\n",
      "1164450/1164450 [==============================] - 209s 179us/sample - loss: 1.2826 - acc: 0.5339 - val_loss: 1.3646 - val_acc: 0.5064\n",
      "Epoch 7/12\n",
      "1164450/1164450 [==============================] - 208s 178us/sample - loss: 1.2814 - acc: 0.5376 - val_loss: 1.3710 - val_acc: 0.5053\n",
      "Epoch 8/12\n",
      "1164450/1164450 [==============================] - 208s 179us/sample - loss: 1.2740 - acc: 0.5414 - val_loss: 1.3837 - val_acc: 0.5047\n",
      "Epoch 9/12\n",
      "1164450/1164450 [==============================] - 207s 177us/sample - loss: 1.2509 - acc: 0.5459 - val_loss: 1.3831 - val_acc: 0.5039\n",
      "Epoch 10/12\n",
      "1164450/1164450 [==============================] - 208s 179us/sample - loss: 1.2422 - acc: 0.5490 - val_loss: 1.3921 - val_acc: 0.5046\n",
      "Epoch 11/12\n",
      "1164450/1164450 [==============================] - 208s 179us/sample - loss: 1.2349 - acc: 0.5522 - val_loss: 1.4026 - val_acc: 0.5008\n",
      "Epoch 12/12\n",
      "1164450/1164450 [==============================] - 207s 177us/sample - loss: 1.2273 - acc: 0.5548 - val_loss: 1.4079 - val_acc: 0.5025\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import ReLU, Softmax\n",
    "from keras.callbacks import CSVLogger\n",
    "import tensorflow.python.keras.backend as K \n",
    "sess = K.get_session()\n",
    "\n",
    "# input layer\n",
    "model_input = Input(shape=(input_shape, ))  # 500\n",
    "X = model_input\n",
    "\n",
    "# 1st hidden layer\n",
    "X_W1 = Dense(units=400)(X)\n",
    "H1 = ReLU()(X_W1)\n",
    "\n",
    "# 2nd hidden layer\n",
    "H1_W2 = Dense(units=200)(H1)\n",
    "H2 = ReLU()(H1_W2)\n",
    "\n",
    "# 3nd hidden layer\n",
    "H2_W3 = Dense(units=100)(H2)\n",
    "H3 = ReLU()(H2_W3)\n",
    "\n",
    "# output layer\n",
    "H3_W4 = Dense(units=output_shape)(H3)\n",
    "H4 = Softmax()(H3_W4)\n",
    "\n",
    "model_output = H4\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "# loss function & optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# show model construction\n",
    "model.summary()\n",
    "\n",
    "csv_logger = CSVLogger('logs/training_log_07.csv')\n",
    "\n",
    "# training setting\n",
    "epochs = 12\n",
    "batch_size = 32\n",
    "# X_train_reorder = X_train.sort_indices\n",
    "# training!\n",
    "history = model.fit(X_train_BOW500_ORI, Y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[csv_logger],\n",
    "                    validation_data = (X_valid_BOW500_ORI, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_result = model.predict(test_bow500_ORI.toarray(), batch_size=128)\n",
    "pd.DataFrame(pred_result).to_csv(\"07_MLP_BOW500_epoch12_ORI.csv\")\n",
    "pd.DataFrame(test_dataset.iloc[:,0]).to_csv(\"INDEX_07_MLP_BOW500_epoch12_ORI.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW2000 + clean DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check label:  ['anger' 'anticipation' 'disgust' 'fear' 'joy' 'sadness' 'surprise'\n",
      " 'trust']\n",
      "\n",
      "## Before convert\n",
      "y_train[0:4]:\n",
      " 1347521    anticipation\n",
      "628192          disgust\n",
      "1365751             joy\n",
      "1262065           trust\n",
      "Name: emotion, dtype: object\n",
      "\n",
      "y_train.shape:  (1164450,)\n",
      "y_test.shape:  (291113,)\n",
      "\n",
      "\n",
      "## After convert\n",
      "y_train[0:4]:\n",
      " [[0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "\n",
      "y_train.shape:  (1164450, 8)\n",
      "y_test.shape:  (291113, 8)\n",
      "input_shape:  2000\n",
      "output_shape:  8\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train_BOW2000)\n",
    "print('check label: ', label_encoder.classes_)\n",
    "print('\\n## Before convert')\n",
    "print('y_train[0:4]:\\n', y_train_BOW2000[0:4])\n",
    "print('\\ny_train.shape: ', y_train_BOW2000.shape)\n",
    "print('y_test.shape: ', y_valid_BOW2000.shape)\n",
    "\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.np_utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "Y_train = label_encode(label_encoder, y_train_BOW2000)\n",
    "Y_test = label_encode(label_encoder, y_valid_BOW2000)\n",
    "\n",
    "print('\\n\\n## After convert')\n",
    "print('y_train[0:4]:\\n', Y_train[0:4])\n",
    "print('\\ny_train.shape: ', Y_train.shape)\n",
    "print('y_test.shape: ', Y_test.shape)\n",
    "\n",
    "# I/O check\n",
    "input_shape = X_train_BOW2000.shape[1]\n",
    "print('input_shape: ', input_shape)\n",
    "\n",
    "output_shape = len(label_encoder.classes_)\n",
    "print('output_shape: ', output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 2000)]            0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 400)               800400    \n",
      "_________________________________________________________________\n",
      "re_lu_6 (ReLU)               (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 200)               80200     \n",
      "_________________________________________________________________\n",
      "re_lu_7 (ReLU)               (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "re_lu_8 (ReLU)               (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 8)                 808       \n",
      "_________________________________________________________________\n",
      "softmax_2 (Softmax)          (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 901,508\n",
      "Trainable params: 901,508\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1164450 samples, validate on 291113 samples\n",
      "Epoch 1/4\n",
      "1164450/1164450 [==============================] - 240s 206us/sample - loss: 1.3538 - acc: 0.5097 - val_loss: 1.3196 - val_acc: 0.5218\n",
      "Epoch 2/4\n",
      "1164450/1164450 [==============================] - 239s 205us/sample - loss: 1.2804 - acc: 0.5363 - val_loss: 1.3088 - val_acc: 0.5272\n",
      "Epoch 3/4\n",
      "1164450/1164450 [==============================] - 239s 205us/sample - loss: 1.2323 - acc: 0.5542 - val_loss: 1.3149 - val_acc: 0.5241\n",
      "Epoch 4/4\n",
      "1164450/1164450 [==============================] - 241s 207us/sample - loss: 1.1881 - acc: 0.5705 - val_loss: 1.3280 - val_acc: 0.5235\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import ReLU, Softmax\n",
    "from keras.callbacks import CSVLogger\n",
    "import tensorflow.python.keras.backend as K \n",
    "sess = K.get_session()\n",
    "\n",
    "# input layer\n",
    "model_input = Input(shape=(input_shape, ))  # 500\n",
    "X = model_input\n",
    "\n",
    "# 1st hidden layer\n",
    "X_W1 = Dense(units=400)(X)\n",
    "H1 = ReLU()(X_W1)\n",
    "\n",
    "# 2nd hidden layer\n",
    "H1_W2 = Dense(units=200)(H1)\n",
    "H2 = ReLU()(H1_W2)\n",
    "\n",
    "# 3nd hidden layer\n",
    "H2_W3 = Dense(units=100)(H2)\n",
    "H3 = ReLU()(H2_W3)\n",
    "\n",
    "# output layer\n",
    "H3_W4 = Dense(units=output_shape)(H3)\n",
    "H4 = Softmax()(H3_W4)\n",
    "\n",
    "model_output = H4\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "# loss function & optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# show model construction\n",
    "model.summary()\n",
    "\n",
    "csv_logger = CSVLogger('logs/training_log_08.csv')\n",
    "\n",
    "# training setting\n",
    "epochs = 4\n",
    "batch_size = 32\n",
    "# X_train_reorder = X_train.sort_indices\n",
    "# training!\n",
    "history = model.fit(X_train_BOW2000, Y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[csv_logger],\n",
    "                    validation_data = (X_valid_BOW2000, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_result = model.predict(test_bow2000.toarray(), batch_size=128)\n",
    "pd.DataFrame(pred_result).to_csv(\"08_MLP_BOW2000_epoch04_ORI.csv\")\n",
    "pd.DataFrame(test_dataset.iloc[:,0]).to_csv(\"INDEX_08_MLP_BOW2000_epoch04_ORI.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW2000 + ORI DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train_BOW500_ORI)\n",
    "print('check label: ', label_encoder.classes_)\n",
    "print('\\n## Before convert')\n",
    "print('y_train[0:4]:\\n', y_train_BOW2000_ORI[0:4])\n",
    "print('\\ny_train.shape: ', y_train_BOW2000_ORI.shape)\n",
    "print('y_test.shape: ', y_valid_BOW2000_ORI.shape)\n",
    "\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.np_utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "Y_train = label_encode(label_encoder, y_train_BOW2000_ORI)\n",
    "Y_test = label_encode(label_encoder, y_valid_BOW2000_ORI)\n",
    "\n",
    "print('\\n\\n## After convert')\n",
    "print('y_train[0:4]:\\n', Y_train[0:4])\n",
    "print('\\ny_train.shape: ', Y_train.shape)\n",
    "print('y_test.shape: ', Y_test.shape)\n",
    "\n",
    "# I/O check\n",
    "input_shape = X_train_BOW2000_ORI.shape[1]\n",
    "print('input_shape: ', input_shape)\n",
    "\n",
    "output_shape = len(label_encoder.classes_)\n",
    "print('output_shape: ', output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 2000)]            0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 400)               800400    \n",
      "_________________________________________________________________\n",
      "re_lu_9 (ReLU)               (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 200)               80200     \n",
      "_________________________________________________________________\n",
      "re_lu_10 (ReLU)              (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "re_lu_11 (ReLU)              (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 8)                 808       \n",
      "_________________________________________________________________\n",
      "softmax_3 (Softmax)          (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 901,508\n",
      "Trainable params: 901,508\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1164450 samples, validate on 291113 samples\n",
      "Epoch 1/4\n",
      "1164450/1164450 [==============================] - 240s 206us/sample - loss: 1.7894 - acc: 0.3550 - val_loss: 1.7910 - val_acc: 0.3525\n",
      "Epoch 2/4\n",
      "1164450/1164450 [==============================] - 248s 213us/sample - loss: 1.7883 - acc: 0.3550 - val_loss: 1.7904 - val_acc: 0.3525\n",
      "Epoch 3/4\n",
      "1164450/1164450 [==============================] - 247s 212us/sample - loss: 1.7881 - acc: 0.3550 - val_loss: 1.7904 - val_acc: 0.3525\n",
      "Epoch 4/4\n",
      "1164450/1164450 [==============================] - 245s 210us/sample - loss: 1.7881 - acc: 0.3550 - val_loss: 1.7909 - val_acc: 0.3525\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import ReLU, Softmax\n",
    "from keras.callbacks import CSVLogger\n",
    "import tensorflow.python.keras.backend as K \n",
    "sess = K.get_session()\n",
    "\n",
    "# input layer\n",
    "model_input = Input(shape=(input_shape, ))  # 500\n",
    "X = model_input\n",
    "\n",
    "# 1st hidden layer\n",
    "X_W1 = Dense(units=400)(X)\n",
    "H1 = ReLU()(X_W1)\n",
    "\n",
    "# 2nd hidden layer\n",
    "H1_W2 = Dense(units=200)(H1)\n",
    "H2 = ReLU()(H1_W2)\n",
    "\n",
    "# 3nd hidden layer\n",
    "H2_W3 = Dense(units=100)(H2)\n",
    "H3 = ReLU()(H2_W3)\n",
    "\n",
    "# output layer\n",
    "H3_W4 = Dense(units=output_shape)(H3)\n",
    "H4 = Softmax()(H3_W4)\n",
    "\n",
    "model_output = H4\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "# loss function & optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# show model construction\n",
    "model.summary()\n",
    "\n",
    "csv_logger = CSVLogger('logs/training_log_09.csv')\n",
    "\n",
    "# training setting\n",
    "epochs = 4\n",
    "batch_size = 32\n",
    "# X_train_reorder = X_train.sort_indices\n",
    "# training!\n",
    "history = model.fit(X_train_BOW2000_ORI, Y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[csv_logger],\n",
    "                    validation_data = (X_valid_BOW2000_ORI, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_result = model.predict(test_bow2000_ORI.toarray(), batch_size=128)\n",
    "pd.DataFrame(pred_result).to_csv(\"09_MLP_BOW2000_ORI_epoch04_ORI.csv\")\n",
    "pd.DataFrame(test_dataset.iloc[:,0]).to_csv(\"INDEX_09_MLP_BOW2000_ORI_epoch04_ORI.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 BI-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tclee/anaconda3/envs/Data_mining/lib/python3.9/site-packages/tensorflow/python/keras/initializers/initializers_v1.py:58: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/tclee/anaconda3/envs/Data_mining/lib/python3.9/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/tclee/anaconda3/envs/Data_mining/lib/python3.9/site-packages/tensorflow/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/tclee/anaconda3/envs/Data_mining/lib/python3.9/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 256)          2560000   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100, 256)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 100, 200)          286400    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200)               241600    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 8)                 1608      \n",
      "=================================================================\n",
      "Total params: 3,089,608\n",
      "Trainable params: 3,089,608\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1164450 samples, validate on 291113 samples\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-24 04:32:33.037479: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164450/1164450 [==============================] - 1309s 1ms/sample - loss: 1.3026 - acc: 0.5299 - val_loss: 1.2530 - val_acc: 0.5464\n",
      "Epoch 2/4\n",
      "1164450/1164450 [==============================] - 1304s 1ms/sample - loss: 1.2300 - acc: 0.5556 - val_loss: 1.2317 - val_acc: 0.5538\n",
      "Epoch 3/4\n",
      "1164450/1164450 [==============================] - 1304s 1ms/sample - loss: 1.2011 - acc: 0.5665 - val_loss: 1.2279 - val_acc: 0.5564\n",
      "Epoch 4/4\n",
      "1164450/1164450 [==============================] - 1307s 1ms/sample - loss: 1.1844 - acc: 0.5722 - val_loss: 1.2284 - val_acc: 0.5565\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "BILSTM_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(10000, 256, input_length=100),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Bidirectional(CuDNNLSTM(100, return_sequences=True)),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Bidirectional(CuDNNLSTM(100)),\n",
    "\n",
    "    tf.keras.layers.Dense(8, activation='softmax')\n",
    "])\n",
    "\n",
    "csv_logger = CSVLogger('logs/training_log_10.csv')\n",
    "\n",
    "BILSTM_model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "BILSTM_model.summary()\n",
    "\n",
    "history_BILSTM = BILSTM_model.fit(\n",
    "                    X_train_keras, y_train_keras,\n",
    "                    epochs=4,\n",
    "                    batch_size=32, \n",
    "                    callbacks=[csv_logger],\n",
    "                    validation_data = (X_valid_keras, y_valid_keras)\n",
    ")\n",
    "\n",
    "BILSTM_model.save('10_BILSTM_epoch4_length_100.h5', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "BILSTM_result = BILSTM_model.predict(X_test_keras, batch_size=32)\n",
    "pd.DataFrame(BILSTM_result).to_csv(\"10_BILSTM_epoch4_length_100.csv\") # including all emotions\n",
    "BILSTM_pred_list = np.argmax(BILSTM_result, axis=1)\n",
    "id_to_emotion = lambda labels: np.array([index_to_emotions.get(x) for x in BILSTM_pred_list])\n",
    "BILSTM_id_to_emotion = id_to_emotion(BILSTM_pred_list)\n",
    "output = test_dataset[['tweet_id']].reset_index(drop=True)\n",
    "output['emotion']=pd.Series(BILSTM_id_to_emotion)\n",
    "output.columns = ['id', 'emotion']\n",
    "output.to_csv('10_result.csv', index=False) # emotion result only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 256)          2560000   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100, 256)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 100, 200)          286400    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 200)               241600    \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 8)                 1608      \n",
      "=================================================================\n",
      "Total params: 3,089,608\n",
      "Trainable params: 3,089,608\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1164450 samples, validate on 291113 samples\n",
      "Epoch 1/4\n",
      "1164450/1164450 [==============================] - 1298s 1ms/sample - loss: 1.2086 - acc: 0.5640 - val_loss: 1.1229 - val_acc: 0.5947\n",
      "Epoch 2/4\n",
      "1164450/1164450 [==============================] - 1295s 1ms/sample - loss: 1.1036 - acc: 0.6017 - val_loss: 1.0977 - val_acc: 0.6040\n",
      "Epoch 3/4\n",
      "1164450/1164450 [==============================] - 1295s 1ms/sample - loss: 1.0709 - acc: 0.6135 - val_loss: 1.0927 - val_acc: 0.6061\n",
      "Epoch 4/4\n",
      "1164450/1164450 [==============================] - 1293s 1ms/sample - loss: 1.0530 - acc: 0.6207 - val_loss: 1.0929 - val_acc: 0.6073\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "BILSTM_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(10000, 256, input_length=100),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Bidirectional(CuDNNLSTM(100, return_sequences=True)),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Bidirectional(CuDNNLSTM(100)),\n",
    "\n",
    "    tf.keras.layers.Dense(8, activation='softmax')\n",
    "])\n",
    "\n",
    "csv_logger = CSVLogger('logs/training_log_11.csv')\n",
    "\n",
    "BILSTM_model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "BILSTM_model.summary()\n",
    "\n",
    "history_BILSTM = BILSTM_model.fit(\n",
    "                    X_train_keras_ORI, y_train_keras_ORI,\n",
    "                    epochs=4,\n",
    "                    batch_size=32, \n",
    "                    callbacks=[csv_logger],\n",
    "                    validation_data = (X_valid_keras_ORI, y_valid_keras_ORI)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "BILSTM_result = BILSTM_model.predict(X_test_keras, batch_size=32)\n",
    "pd.DataFrame(BILSTM_result).to_csv(\"11_BILSTM_ORI_epoch4_length_100.csv\")\n",
    "BILSTM_pred_list = np.argmax(BILSTM_result, axis=1)\n",
    "id_to_emotion = lambda labels: np.array([index_to_emotions.get(x) for x in BILSTM_pred_list])\n",
    "BILSTM_id_to_emotion = id_to_emotion(BILSTM_pred_list)\n",
    "output = test_dataset[['tweet_id']].reset_index(drop=True)\n",
    "output['emotion']=pd.Series(BILSTM_id_to_emotion)\n",
    "output.columns = ['id', 'emotion']\n",
    "output.to_csv('11_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Report\n",
    "\n",
    "I've aleady re-build this jupyter notebook for typesetting, so some value may be different.\n",
    "\n",
    "## 1 Compare the result\n",
    "\n",
    "(all score are based on kaggle)\n",
    "\n",
    "- Different Classification model with tokenizer\n",
    "\n",
    "    - Decision Tree + BOW500(tokenizer=nltk.word_tokenize)\n",
    "\n",
    "        The score of this is 0.18764 only, so I give it up in early stage.\n",
    "\n",
    "    - Random Forest + BOW500(tokenizer=nltk.word_tokenize)\n",
    "\n",
    "        The score of this is 0.30764.\n",
    "\n",
    "        Firstly, I think it's a good direction to dig into. However, when I check the output, I found that it outputs \"joy\" to all tweats. It's obviously abnormal, so I give up this way.\n",
    "\n",
    "    - Multinomial Naive Bayes + TFIDF(default tokenizer)\n",
    "\n",
    "        The score with clean data is 0.30083, and the score with original data is 0.31127.\n",
    "\n",
    "        In this case, I found that the original data's performance is better than the data I cleaned. I worried about that I clean some meaningful features, but the case I did I thaught is normal. Therefore, in all the next steps, I will also implement 2 kinds of data to predict the result.\n",
    "\n",
    "    - XGB Classifier + TFIDF(default tokenizer)\n",
    "\n",
    "        The score with clean data is 0.29176, because it performed wroser than MNB, and it requires 2 hours to train, so I give up test it on original data.\n",
    "\n",
    "        I found this method on kaggle, and many people consider it as a state-of-the-art way to competiotion, but it seems that I can't control it.\n",
    "\n",
    "    -  MLP + BOW500/2000(tokenizer=nltk.word_tokenize)\n",
    "\n",
    "        For BOW500, the score with clean data is 0.41512, and the score with original data is 0.43921.\n",
    "\n",
    "        For BOW2000, the score with clean data is 0.4168, and the score with original data is 0.45372.\n",
    "\n",
    "        The model I build can refer to the summary of the cell, or describe below:\n",
    "\n",
    "        input->Dense400+Relu->Dense200+Relu->Dense100+Relu->Dense8+softmax->output\n",
    "\n",
    "        I found that if we create more bags of words, it seems that it can enhance the performance.\n",
    "\n",
    "        Nonetheless, when I tried BOW3000 of original data, it comes out that the score is 0.43921, it's worsen.\n",
    "\n",
    "        Besides, the overfitting is coming out at epoch 4. I tried the BOW2000+original data in epoch 4 and 12 respectively, it comes out the result is 0.45372 and 0.43731.\n",
    "\n",
    "        ![Training Loss per epoch in BOW2000](img/training_log_epoch12.png)\n",
    "\n",
    "        Therefore, in my case, MLP+BOW2000 at epoch 4 is the best result.\n",
    "\n",
    "    -  BILSTM + keras.tokenizer(tokenizer=nltk.word_tokenize)\n",
    "\n",
    "        I found some competiotion report that uses BI-LSTM as a powerful tool, so I tried to implement it.\n",
    "\n",
    "        Because in the above test, I found that original data's performance is better than my clean data, I only tried on original data.\n",
    "\n",
    "        For default length + original data, the score is *0.48345, which is my highest score.*\n",
    "\n",
    "        For length 400, the score is 0.43641, for length 100, the score is 0.43653. Therefore, it seems that the difference is not obvious.\n",
    "\n",
    "        Conclusively, due to lack of time, I still have trouble controling this powerful method, it's quite unfortunately.\n",
    "\n",
    "\n",
    "## 2 The bad situation I faced\n",
    "\n",
    "- Missinput the wrong index label\n",
    "\n",
    "    It consumes me many time to figure out why my grade is so slow; eventually, when I tried to sort them to compare the result, I found that the tweat_id of my test_dataset are different in different kernal.\n",
    "\n",
    "    While I fixed this bug, the grade becomes normal, but it consumes me lots of time to re-train the data to fix it.\n",
    "\n",
    "\n",
    "## 3 The thing I can improve\n",
    "\n",
    "- Comprehensive the parameter of BI-LSTM to improve it.\n",
    "\n",
    "- Try on specific NLP model. (i.e., BERT, RoBertA)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('selenium')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "22b88c29f3c4ae317522438c45be4d1123b8810926beaba56f8ed4a7d0c6bccc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
